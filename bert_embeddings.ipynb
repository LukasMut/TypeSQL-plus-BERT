{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T18:41:38.374807Z",
     "start_time": "2019-09-08T18:41:21.659426Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch \n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_transformers import *\n",
    "from typesql.utils import *\n",
    "from retokenizer import Retokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T18:42:13.539929Z",
     "start_time": "2019-09-08T18:41:48.246377Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from original dataset\n",
      "Loading data from data/train_tok.jsonl\n",
      "Loading data from data/train_tok.tables.jsonl\n",
      "Loading data from data/dev_tok.jsonl\n",
      "Loading data from data/dev_tok.tables.jsonl\n",
      "Loading data from data/test_tok.jsonl\n",
      "Loading data from data/test_tok.tables.jsonl\n"
     ]
    }
   ],
   "source": [
    "sql_data, table_data, val_sql_data, val_table_data, \\\n",
    "            test_sql_data, test_table_data, \\\n",
    "            TRAIN_DB, DEV_DB, TEST_DB = load_dataset(use_small=False) # False (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:25:02.670818Z",
     "start_time": "2019-09-08T08:25:02.666807Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_sql_data(sql_data_train, sql_data_val):\n",
    "    sql_data_train.extend(sql_data_val)\n",
    "    return sql_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:26:37.072744Z",
     "start_time": "2019-09-07T07:26:37.065725Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def count_context_toks(tok = 'the'): \n",
    "    unique_toks = set()\n",
    "    for sent_id in sent_idxs:\n",
    "        string = tokenizer.decode(sent_id[0])\n",
    "        string = string.split()\n",
    "        if tok in string:\n",
    "            idx = string.index(tok)\n",
    "            unique_toks.add(sent_id[0][idx])\n",
    "    return len(unique_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:27:52.708694Z",
     "start_time": "2019-09-08T08:27:52.703682Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_questions(sql_data, tokenize = True):\n",
    "    key = 'question_tok' if tokenize else 'question'\n",
    "    return list(map(lambda el:el[key], sql_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:30:14.994715Z",
     "start_time": "2019-09-08T08:30:14.985689Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_preprocessing(questions, flatten = False):\n",
    "    \"\"\"\n",
    "        Args: Raw natural language questions represented as strings. \n",
    "        Computation: Sentence preprocessing steps necessary for BERT model.\n",
    "                    Each sentence is required to be preceded by a special [CLS] token\n",
    "                    and followed by a special [SEP] token.\n",
    "                    Token IDs arrays have to be converted into tensors \n",
    "                    before they can be passed to BERT. \n",
    "        Return: tokenized questions, token IDs, segment IDs (i.e., ones),\n",
    "                tuples of (tokens, ids) either per token-id-pair or as a list per sentence.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    questions = list(map(lambda q: '[CLS]' + ' ' + ' '.join(q) + ' ' +'[SEP]', questions))\n",
    "    tok_questions = [tokenizer.tokenize(q) for q in questions]\n",
    "    indexed_tokens = [torch.tensor([tokenizer.convert_tokens_to_ids(tok_q)], dtype=torch.long) \\\n",
    "                      for tok_q in tok_questions]\n",
    "    segment_ids = [torch.tensor([np.ones(len(q), dtype=int)],dtype=torch.long) \\\n",
    "                   for q in tok_questions]\n",
    "    \n",
    "    idx2word = {idx.item(): tok_w for tok_q, indexes in zip(tok_questions, indexed_tokens) \\\n",
    "                for tok_w, idx in zip(tok_q, indexes[0])}\n",
    "    \n",
    "    return tok_questions, indexed_tokens, segment_ids, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:45:39.310602Z",
     "start_time": "2019-09-04T05:45:39.293556Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: investigate how you can make this function work properly \n",
    "#FOR NOW: just use token representations of last hidden layer (implemented in cell below)\n",
    "def get_summed_embeddings(model, toks_ids, segment_ids):\n",
    "    \"\"\"\n",
    "        Input: BertModel, token id tensors, segment id tensors\n",
    "        Computation: Convert the hidden state embeddings into single token vectors\n",
    "                     Holds the list of 12 layer embeddings for each token\n",
    "                     Will have the shape: [# tokens, # layers, # features]\n",
    "        Output: Bert context embedding for each token in the question.\n",
    "                Final token embedding is the sum over the last four hidden layer representions.\n",
    "    \"\"\"\n",
    "    # encoded_layers = model(toks_ids, segment_ids)[0]\n",
    "    encoded_layers, _ = model(toks_ids)[-2:]\n",
    "    token_embeddings = np.zeros((len(toks_ids[0]), 768), dtype=float)\n",
    "    hidden_size = 12\n",
    "    batch_i = 0\n",
    "    for token_i in range(len(toks_ids[0])):\n",
    "        # 12 layers of hidden states for each token\n",
    "        hidden_layers = np.zeros((hidden_size, 768), dtype=float) \n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            hidden_layers[layer_i] = encoded_layers[layer_i][batch_i][token_i]\n",
    "        # each token's embedding is represented as a sum of the last four hidden layers\n",
    "        token_embeddings[token_i] = torch.sum(torch.stack(hidden_layers[token_i])[-4:], 0).numpy()\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:32:38.760991Z",
     "start_time": "2019-09-08T08:32:38.750964Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_token_ids(sql_data, bert_questions, bert_ids, arbitrary_id = 99999):\n",
    "    rejoined_toks = list()\n",
    "    rejoined_ids = list()\n",
    "    retokenizer = Retokenizer(merge=None, embeddings=False)\n",
    "    for i, (question, bert_question, bert_id) in enumerate(zip(sql_data, bert_questions, bert_ids)):\n",
    "        bert_id = list(bert_id[0].numpy())\n",
    "        new_toks, new_ids, new_id = retokenizer.retokenize(question['question_tok'], bert_question[1:-1], bert_id[1:-1], arbitrary_id)\n",
    "        arbitrary_id = new_id\n",
    "        rejoined_toks.append(new_toks)\n",
    "        rejoined_ids.append(new_ids)\n",
    "    return rejoined_toks, rejoined_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:38:57.319342Z",
     "start_time": "2019-09-08T08:38:57.304301Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_embeddings(bert_questions, bert_ids, segment_ids, sql_data, merge, arbitrary_id = 99999):\n",
    "    \"\"\"\n",
    "        Args: WordPiece tokenized questions, torch tensors of token ids and segment ids and SQL data.\n",
    "        Computation: load pre-trained BERT model (weights),\n",
    "                     put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "                     \"torch.no_grad()\" deactivates the gradient calculations, \n",
    "                     saves memory, and speeds up computation (we don't need gradients or backprop).\n",
    "        Return: dictionary that maps token ids (keys) \n",
    "                to their corresponding BERT context embeddings (values).\n",
    "    \"\"\"\n",
    "    #TODO: check what's necessary to ouput all hidden states\n",
    "    #model = BertModel.from_pretrained('bert-base-uncased',\n",
    "    #                              output_hidden_states=True,\n",
    "    #                              output_attentions=True)\n",
    "    \n",
    "    model = BertModel.from_pretrained('bert-base-uncased')    \n",
    "    model.eval()\n",
    "    id2embed = dict()\n",
    "    id2tok = dict()\n",
    "    rejoined_toks = list()\n",
    "    rejoined_ids = list()\n",
    "    retokenizer = Retokenizer(merge=merge, embeddings=True)\n",
    "    with torch.no_grad():\n",
    "        for i, (question, bert_question, bert_id, segment_id) in enumerate(zip(sql_data, bert_questions, bert_ids, segment_ids)):\n",
    "            bert_embeddings = model(bert_id, segment_id)[0][0]\n",
    "            bert_embeddings = np.array(list(map(lambda embedding:embedding.numpy(), bert_embeddings)))\n",
    "\n",
    "            #TODO: make \"get_summed_embeddings\" function work\n",
    "            #token_embeddings = get_summed_embeddings(model, tok_id, segment_id)\n",
    "            \n",
    "            bert_id = list(bert_id[0].numpy())\n",
    "            \n",
    "            if bert_id[0] not in id2tok:\n",
    "                id2tok[bert_id[0]]=bert_question[0]\n",
    "            if bert_id[-1] not in id2tok:\n",
    "                id2tok[bert_id[-1]]=bert_question[-1]\n",
    "            if bert_id[0] not in id2embed:\n",
    "                id2embed[bert_id[0]]=bert_embeddings[0]\n",
    "            if bert_id[-1] not in id2embed:\n",
    "                id2embed[bert_id[-1]]=bert_embeddings[-1]\n",
    "                \n",
    "            new_toks, new_ids, new_embeddings, new_id = retokenizer.retokenize(question['question_tok'], \n",
    "                                                                               bert_question[1:-1], \n",
    "                                                                               bert_id[1:-1], \n",
    "                                                                               arbitrary_id,\n",
    "                                                                               bert_embeddings[1:-1])\n",
    "            \n",
    "            arbitrary_id = new_id   \n",
    "            try:\n",
    "                assert len(new_ids) == len(new_toks) == len(new_embeddings) == len(question['question_tok'])\n",
    "                for tok_id, bert_tok, bert_embedding in zip(new_ids, new_toks, new_embeddings):\n",
    "                    if tok_id not in id2embed:\n",
    "                        id2embed[tok_id] = np.array(bert_embedding)\n",
    "                    if tok_id not in id2tok:\n",
    "                        id2tok[tok_id] = bert_tok\n",
    "            except AssertionError:\n",
    "                pass\n",
    "            \n",
    "            rejoined_toks.append(new_toks)\n",
    "            rejoined_ids.append(new_ids)\n",
    "            \n",
    "    return rejoined_toks, rejoined_ids, id2embed, id2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:39:56.858539Z",
     "start_time": "2019-09-08T08:39:56.851520Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def drop_data(tok_questions, tok_ids, sql_data, idx_to_drop):\n",
    "    k = 0\n",
    "    n_errors = len(idx_to_drop)\n",
    "    n_questions = len(sql_data)\n",
    "    for idx in idx_to_drop:\n",
    "        tok_questions.pop(idx-k)\n",
    "        tok_ids.pop(idx-k)\n",
    "        sql_data.pop(idx-k)\n",
    "        k += 1\n",
    "    assert len(sql_data) == n_questions-n_errors, 'incorrect number of erroneous questions was dropped'\n",
    "    return tok_questions, tok_ids, sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T21:16:18.379626Z",
     "start_time": "2019-09-07T21:16:18.370601Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_sql_data(sql_data):\n",
    "    \"\"\"\n",
    "        Input: SQL dataset\n",
    "        Output: Updated SQL dataset with bert tokens and corresponding bert ids\n",
    "                BERT tokens were rejoined into TypeSQL's gold standard tokens and\n",
    "                hence are the same\n",
    "    \"\"\"\n",
    "    bert_questions, bert_ids, _, _ = bert_preprocessing(extract_questions(sql_data))\n",
    "    tok_questions, tok_ids = bert_token_ids(sql_data, bert_questions, bert_ids)\n",
    "    idx_to_pop = list()\n",
    "    n_original_questions = len(sql_data)\n",
    "    print(\"Number of questions before computing BERT token representations:\", n_original_questions)\n",
    "    for i, (question, tok_id, tok_question) in enumerate(zip(sql_data, tok_ids, tok_questions)):\n",
    "        try:\n",
    "            assert len(question['question_tok']) == len(tok_id)  == len(tok_question)\n",
    "        except:\n",
    "            idx_to_pop.append(i)\n",
    "    \n",
    "    if len(idx_to_pop) > 0:   \n",
    "        tok_questions, tok_ids, sql_data = drop_data(tok_questions, tok_ids, sql_data, idx_to_pop) \n",
    "    \n",
    "    for i, (question, tok_id, tok_question) in enumerate(zip(sql_data, tok_ids, tok_questions)):\n",
    "        try:\n",
    "            assert len(sql_data[i]['question_tok']) == len(tok_id)  == len(tok_question)\n",
    "            sql_data[i]['bert_tokenized_question'] = tok_question\n",
    "            sql_data[i]['bert_token_ids'] = tok_id #list(tok_id[0].numpy())\n",
    "        except:\n",
    "            raise Exception(\"Removing incorrectly rejoined questions did not work. Check function!\")\n",
    "    \n",
    "    n_removed_questions = n_original_questions-len(sql_data)\n",
    "    \n",
    "    print(\"Number of questions in pre-processed dataset (after rejoining):\", len(sql_data))\n",
    "    print(\"Questions that could not be rejoined into TypeSQL tokens:\", n_removed_questions)\n",
    "    print(\"{}% of the original questions were removed\".format(round((n_removed_questions / n_original_questions)*100, 2)))\n",
    "    print(\"SQL data has been updated with BERT ids (tokens are the same as TypeSQL's tokens)...\")\n",
    "    return sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T16:12:21.238962Z",
     "start_time": "2019-09-07T16:03:29.137Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: figure out whether this function is actually necessary (maybe you want to keep those questions)\n",
    "def remove_nonequal_questions(sql_data):\n",
    "    count = 0\n",
    "    for i, question in enumerate(sql_data):\n",
    "        try:\n",
    "            assert question['question_tok'] == question['bert_tokenized_question']\n",
    "        except AssertionError:\n",
    "            sql_data.pop(i)\n",
    "            #print(question['question_tok'])\n",
    "            #print()\n",
    "            #print(question['bert_tokenized_question'])\n",
    "            #print()\n",
    "            count += 1\n",
    "    print(\"{} questions had different tokens and thus were removed from dataset\".format(count))\n",
    "    return sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:56:19.916243Z",
     "start_time": "2019-09-07T15:56:18.569583Z"
    }
   },
   "outputs": [],
   "source": [
    "sql_data_updated = update_sql_data(sql_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:56:54.061884Z",
     "start_time": "2019-09-07T15:56:54.053859Z"
    }
   },
   "outputs": [],
   "source": [
    "#sql_data = remove_nonequal_questions(sql_data_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:57:30.443577Z",
     "start_time": "2019-09-07T15:57:29.023859Z"
    }
   },
   "outputs": [],
   "source": [
    "val_sql_data_updated = update_sql_data(val_sql_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:58:05.505412Z",
     "start_time": "2019-09-07T15:58:05.497391Z"
    }
   },
   "outputs": [],
   "source": [
    "#val_sql_data = remove_nonequal_questions(val_sql_data_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:58:44.124872Z",
     "start_time": "2019-09-07T15:58:42.782669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sql_data_updated = update_sql_data(test_sql_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:59:21.101840Z",
     "start_time": "2019-09-07T15:59:21.093822Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_sql_data = remove_nonequal_questions(test_sql_data_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:40:08.918474Z",
     "start_time": "2019-09-08T08:40:08.913463Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_pipeline(sql_data_train, sql_data_val, merge='max'):\n",
    "    sql_data = concatenate_sql_data(sql_data_train, sql_data_val)\n",
    "    tok_questions, tok_ids, segment_ids, _ = bert_preprocessing(extract_questions(sql_data))\n",
    "    _, _, id2embed, id2tok = bert_embeddings(tok_questions, tok_ids, segment_ids, sql_data, merge)\n",
    "    assert len(id2embed) == len(id2tok)\n",
    "    return id2tok, id2embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:02:08.665182Z",
     "start_time": "2019-09-08T19:02:08.648139Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def save_embeddings_as_json(id2tok, id2embed, merge, dim):\n",
    "    # np.arrays have to be converted into lists to be .json serializable\n",
    "    id2embed = {int(idx):embedding.tolist() for idx, embedding in id2embed.items()}\n",
    "    id2tok = {int(idx):tok for idx, tok in id2tok.items()}\n",
    "    if merge == 'max':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedMaxFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedMax600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedMax100.json'\n",
    "        ids = './bert/id2tokMax.json'\n",
    "    elif merge == 'avg':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedMeanFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedMean600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedMean100.json'\n",
    "        ids = './bert/id2tokMean.json'\n",
    "    elif merge == 'sum':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedSumFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedSum600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedSum100.json'\n",
    "        ids = './bert/id2tokSum.json' \n",
    "    else:\n",
    "        raise Exception('Embeddings have to be max-pooled, averaged or summed')\n",
    "    with open(embeddings, 'w') as json_file:\n",
    "        json.dump(id2embed, json_file)\n",
    "    with open(ids, 'w') as json_file:\n",
    "        json.dump(id2tok, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T18:45:31.856401Z",
     "start_time": "2019-09-08T18:45:31.850383Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_bert_dicts(file_tok, file_emb):\n",
    "    with open(file_tok) as f:\n",
    "        id2tok = json.loads(f.read())\n",
    "    with open(file_emb) as f:\n",
    "        id2embed = json.loads(f.read())\n",
    "    id2tok = {int(idx):tok for idx, tok in id2tok.items()}\n",
    "    id2embed = {int(idx):np.array(embedding) for idx, embedding in id2embed.items()}\n",
    "    assert len(id2tok) == len(id2embed)\n",
    "    return id2tok, id2embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:12:01.654355Z",
     "start_time": "2019-09-08T19:12:01.642323Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reduce_dimensionality(id2embed, dims_to_keep:int):\n",
    "    \"\"\"\n",
    "        Args: id2embedding dict; number of dimensions to keep (of original 768 BERT embeddings)\n",
    "        Return: id2embdding dict with reduced dimensionality embeddings specified by dims-to-keep \n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    embeddings = np.zeros((len(id2embed), 768))\n",
    "    id2embed = dict(sorted(id2embed.items(), key=lambda kv:kv[0], reverse=False))\n",
    "    for i, (idx, embedding) in enumerate(id2embed.items()):\n",
    "        ids.append(idx)\n",
    "        embeddings[i] = embedding\n",
    "        \n",
    "    pca = PCA(n_components=dims_to_keep, svd_solver='auto', random_state=42)\n",
    "    embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    id2embed_reduced = {idx:embedding for idx, embedding in zip(ids, embeddings)}\n",
    "    return id2embed_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T10:28:22.892988Z",
     "start_time": "2019-09-08T08:40:25.135411Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tok, id2embed = bert_pipeline(sql_data, val_sql_data, merge='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:04:38.540501Z",
     "start_time": "2019-09-08T19:02:14.407892Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, save embeddings with full (768) dimensionality\n",
    "save_embeddings_as_json(id2tok, id2embed, merge='max', dim='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:12:45.509698Z",
     "start_time": "2019-09-08T19:12:09.206651Z"
    }
   },
   "outputs": [],
   "source": [
    "id2embed600 = reduce_dimensionality(id2embed, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:15:34.805718Z",
     "start_time": "2019-09-08T19:13:16.046873Z"
    }
   },
   "outputs": [],
   "source": [
    "# second, save embeddings with reduced (e.g., 600) dimensionality\n",
    "save_embeddings_as_json(id2tok, id2embed600, merge='max', dim=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:15:48.232732Z",
     "start_time": "2019-09-08T19:15:40.518268Z"
    }
   },
   "outputs": [],
   "source": [
    "id2embed100 = reduce_dimensionality(id2embed, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:16:15.665996Z",
     "start_time": "2019-09-08T19:15:56.787124Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# second, save embeddings with reduced (e.g., 100) dimensionality\n",
    "save_embeddings_as_json(id2tok, id2embed100, merge='max', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T19:17:38.316791Z",
     "start_time": "2019-09-08T19:16:44.937416Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tok, id2embedMaxFull = load_bert_dicts(\"./bert/id2tokMax.json\", \"./bert/id2embedMaxFull.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T11:19:12.624976Z",
     "start_time": "2019-09-04T11:18:55.133942Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tokavg, id2embedAvgFull = load_bert_dicts(\"./bert/id2tokMean.json\", \"./bert/id2embedMeanFull.json\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
