{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T13:23:48.994819Z",
     "start_time": "2019-09-05T13:23:23.786529Z"
    }
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import json\n",
    "import re\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_transformers import *\n",
    "from typesql.utils import *\n",
    "from retokenizer import Retokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T13:24:05.174321Z",
     "start_time": "2019-09-05T13:23:49.024899Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from original dataset\n",
      "Loading data from data/train_tok.jsonl\n",
      "Loading data from data/train_tok.tables.jsonl\n",
      "Loading data from data/dev_tok.jsonl\n",
      "Loading data from data/dev_tok.tables.jsonl\n",
      "Loading data from data/test_tok.jsonl\n",
      "Loading data from data/test_tok.tables.jsonl\n"
     ]
    }
   ],
   "source": [
    "sql_data, table_data, val_sql_data, val_table_data, \\\n",
    "            test_sql_data, test_table_data, \\\n",
    "            TRAIN_DB, DEV_DB, TEST_DB = load_dataset(use_small=False) # False (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:43:40.717866Z",
     "start_time": "2019-09-04T05:43:40.712854Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_sql_data(sql_data_train, sql_data_val):\n",
    "    sql_data_train.extend(sql_data_val)\n",
    "    return sql_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:43:57.309850Z",
     "start_time": "2019-09-04T05:43:57.301829Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def count_context_toks(tok = 'the'): \n",
    "    unique_toks = set()\n",
    "    for sent_id in sent_idxs:\n",
    "        string = tokenizer.decode(sent_id[0])\n",
    "        string = string.split()\n",
    "        if tok in string:\n",
    "            idx = string.index(tok)\n",
    "            unique_toks.add(sent_id[0][idx])\n",
    "    return len(unique_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:44:14.242869Z",
     "start_time": "2019-09-04T05:44:14.234848Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_questions(sql_data, tokenize = True):\n",
    "    key = 'question_tok' if tokenize else 'question'\n",
    "    return list(map(lambda el:el[key], sql_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:44:31.766458Z",
     "start_time": "2019-09-04T05:44:31.740389Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_preprocessing(questions, tok2ids_tuple = False, flatten = False):\n",
    "    \"\"\"\n",
    "        Input: Raw natural language questions represented as strings. \n",
    "        Computation: Sentence preprocessing steps necessary for BERT model.\n",
    "                    Each sentence is required to be preceded by a special [CLS] token\n",
    "                    and followed by a special [SEP] token.\n",
    "                    Token IDs arrays have to be converted into tensors \n",
    "                    before they can be passed to BERT. \n",
    "        Output: tokenized questions, token IDs, segment IDs (i.e., ones),\n",
    "                tuples of (tokens, ids) either per token-id-pair or as a list per sentence.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained('roberta-base', add_special_tokens=True)\n",
    "    questions = list(map(lambda q: '[CLS]' + ' ' + ' '.join(q) + ' ' +'[SEP]', questions))\n",
    "    #questions = list(map(lambda q: ' '.join(q), questions))\n",
    "    tok_questions = [tokenizer.tokenize(q) for q in questions]\n",
    "    #tok_questions = [rejoin_toks(q) for q in tok_questions]\n",
    "    indexed_tokens = [torch.tensor([tokenizer.convert_tokens_to_ids(tok_q)], dtype=torch.long) \\\n",
    "                      for tok_q in tok_questions]\n",
    "    segment_ids = [torch.tensor([np.ones(len(q), dtype=int)],dtype=torch.long) \\\n",
    "                   for q in tok_questions]\n",
    "    \n",
    "    idx2word = {idx.item(): tok_w for tok_q, indexes in zip(tok_questions, indexed_tokens) \\\n",
    "                for tok_w, idx in zip(tok_q, indexes[0])}\n",
    "    \n",
    "    return tok_questions, indexed_tokens, segment_ids, idx2word\n",
    "    \n",
    "    #if tok2ids_tuple:\n",
    "    #    if flatten:\n",
    "    #        tok_ids = [(tok_w, idx) for tok_q, indexes in zip(tok_questions, indexed_tokens) \\\n",
    "    #                   for tok_w, idx in zip(tok_q, indexes[0])]\n",
    "    #    else:\n",
    "    #        tok_ids = list()\n",
    "    #        for tok_q, ids in zip(tok_questions, indexed_tokens):\n",
    "    #            tok_ids.append(list(map(lambda tok_id: (tok_id[0], tok_id[1]), zip(tok_q, ids[0]))))\n",
    "    #            \n",
    "    #    return tok_questions, indexed_tokens, segment_ids, idx2word, tok_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:44:51.160588Z",
     "start_time": "2019-09-04T05:44:51.146551Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_chunk(indexes, span, i):\n",
    "    start = indexes[-1] if len(indexes) > 0 else 0\n",
    "    if isinstance(span[i-1], int):\n",
    "        chunk = [tok_id for tok_id in span[start:i+1]]\n",
    "        if isinstance(span[start-1], list):\n",
    "            if abs(span[start-1][-1]-span[start]) > 1:\n",
    "                chunk.insert(0, min(chunk)-1)\n",
    "        else:\n",
    "            if abs(span[start-1]-span[start]) > 1 or span[start-1] == span[i]:\n",
    "                chunk.insert(0, min(chunk)-1)\n",
    "    else:\n",
    "        if abs(span[start-1][-1]-span[start]) == 1:\n",
    "            chunk = [span[i]]\n",
    "        else:\n",
    "            chunk = [span[i]-1, span[i]]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:45:07.647420Z",
     "start_time": "2019-09-04T05:45:07.611325Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def chunk_indexes(span):    \n",
    "    chunks = list()\n",
    "    indexes = list()\n",
    "    if len(span) == 1 and isinstance(span[0], int):\n",
    "        span.insert(0, span[0]-1)\n",
    "        return [span]\n",
    "    elif len(span) == 1 and isinstance(span[0], list):\n",
    "        span = span[0]\n",
    "        span.insert(0, span[0]-1)\n",
    "        return [span]\n",
    "    else:\n",
    "        for i, tok_id in enumerate(span): \n",
    "            if i > 0:\n",
    "                if isinstance(tok_id, list) and i < len(span)-1:\n",
    "                    chunks.append(tok_id)\n",
    "                    indexes.append(i+1)\n",
    "                elif isinstance(tok_id, list) and i == len(span)-1:\n",
    "                    chunks.append(tok_id)\n",
    "                else:\n",
    "                    try:\n",
    "                        if isinstance(span[i-1], int) and isinstance(span[i+1], int):\n",
    "                            if (tok_id-span[i-1] == 1) and (abs(tok_id-span[i+1]) > 1):\n",
    "                                chunk = get_chunk(indexes, span, i)\n",
    "                                chunks.append(chunk)\n",
    "                                indexes.append(i+1)\n",
    "                            elif (abs(tok_id-span[i-1]) > 1) and (abs(tok_id-span[i+1]) > 1):\n",
    "                                chunks.append([tok_id-1, tok_id])\n",
    "                                indexes.append(i+1)\n",
    "                        elif isinstance(span[i-1], int) and isinstance(span[i+1], list):\n",
    "                            if (tok_id-span[i-1] == 1):\n",
    "                                chunk = get_chunk(indexes, span, i)\n",
    "                                chunks.append(chunk)\n",
    "                                indexes.append(i+1)\n",
    "                            else:\n",
    "                                chunks.append([tok_id-1, tok_id])\n",
    "                                indexes.append(i+1)\n",
    "                        elif isinstance(span[i-1], list) and isinstance(span[i+1], int):\n",
    "                            if (abs(tok_id-span[i+1]) > 1):\n",
    "                                chunk = get_chunk(indexes, span, i)\n",
    "                                chunks.append(chunk)\n",
    "                                indexes.append(i+1)\n",
    "                    except IndexError: # we are at the end of the list\n",
    "                        if isinstance(span[i-1], int):\n",
    "                            if (tok_id-span[i-1] == 1):\n",
    "                                chunk = get_chunk(indexes, span, i)\n",
    "                                chunks.append(chunk)\n",
    "                            else:\n",
    "                                chunks.append([tok_id-1, tok_id])\n",
    "                        else:\n",
    "                            chunk = get_chunk(indexes, span, i)\n",
    "                            chunks.append(chunk)\n",
    "            else:\n",
    "                if isinstance(tok_id, list):\n",
    "                    tok_id.insert(0, tok_id[0]-1)\n",
    "                    indexes.append(i+1)\n",
    "                else:\n",
    "                    if isinstance(span[i+1], list) or abs(tok_id-span[i+1]) > 1:\n",
    "                        chunks.append([tok_id-1, tok_id])\n",
    "                        indexes.append(i+1)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:45:23.431385Z",
     "start_time": "2019-09-04T05:45:23.381253Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge_token_ids_embed(bert_toks, bert_ids, arbitrary_id, merge, bert_embeddings=list()):\n",
    "    month_names = list(map(lambda x: calendar.month_name[x].lower(), range(1,13)))\n",
    "    bert_toks = bert_toks[1:-1]\n",
    "    bert_ids = bert_ids[1:-1]\n",
    "\n",
    "    if len(bert_embeddings) > 0:\n",
    "        bert_embeddings = bert_embeddings[1:-1]\n",
    "        retokenizer = Retokenizer(merge, embeddings=True)\n",
    "    else:\n",
    "        retokenizer = Retokenizer(merge, embeddings=False)\n",
    "        \n",
    "    ids_to_rejoin = list()\n",
    "    for i, bert_tok in enumerate(bert_toks):  \n",
    "        if i < len(bert_toks)-1:\n",
    "            if re.search(r'#+\\w+', bert_tok) and not re.search(r'#+\\w+', bert_toks[i-1]) and re.search(r'#+\\w+', bert_toks[i-2]) and not re.search(r'#+\\w+', bert_toks[i+1]):\n",
    "                ids_to_rejoin.append([i-1, i])\n",
    "            elif re.search(r'#+\\w+', bert_tok):\n",
    "                ids_to_rejoin.append(i)\n",
    "            if re.search(r\"'\", bert_tok) and (re.search(r'what|who|why|where|how', bert_toks[i-1]) or re.search(r'^s$', bert_toks[i+1])):\n",
    "                ids_to_rejoin.append(i+1)\n",
    "            elif re.search(r\"\\.\", bert_tok) and (re.search(r\"[a-zA-Z]+\", bert_toks[i-1]) and re.search(r\"\\w+\", bert_toks[i+1])):\n",
    "                ids_to_rejoin.append(i)\n",
    "            elif re.search(r\"'\", bert_tok) and not (re.search(r'what|who|why|where|how', bert_toks[i-1])  or re.search(r'^(s|\\?)$', bert_toks[i+1])):\n",
    "                ids_to_rejoin.append(i)\n",
    "                ids_to_rejoin.append(i+1)\n",
    "            elif re.search(r\"\\.|-|/\", bert_tok) and not re.search(r'^\\?$', bert_toks[i+1]):\n",
    "                ids_to_rejoin.append(i)\n",
    "                ids_to_rejoin.append(i+1)   \n",
    "            try:\n",
    "                if re.search(r\",|:\", bert_tok) and (re.search(r\"[0-9]+\", bert_toks[i-1]) and re.search(r\"[0-9]+\", bert_toks[i+1]) and re.search(r\",\", bert_toks[i+2]) and re.search(r\"[0-9]+\", bert_toks[i+3])):\n",
    "                    ids_to_rejoin.append(i)\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "                    ids_to_rejoin.append(i+2)\n",
    "                    ids_to_rejoin.append(i+3)                                                                                                                                                                                                                 \n",
    "                elif re.search(r\",|:\", bert_tok) and re.search(r\"[0-9]+\", bert_toks[i-1]) and re.search(r\"[0-9]+\", bert_toks[i+1]) and not ((re.search(r\",\", bert_toks[i+2]) and re.search(r\"[0-9]+\", bert_toks[i+3])) or (re.search(r\",\", bert_toks[i-2]) and re.search(r\"[0-9]+\", bert_toks[i-3])) or bert_toks[i-2] in month_names or (re.search(r'-', bert_toks[i-2]) and re.search(r'-', bert_toks[i+2]))):\n",
    "                    ids_to_rejoin.append(i)\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "            except IndexError:\n",
    "                if re.search(r\",|:\", bert_tok) and re.search(r\"[0-9]+\", bert_toks[i-1]) and re.search(r\"[0-9]+\", bert_toks[i+1]) and not (bert_toks[i-2] in month_names or re.search(r'-', bert_toks[i-2])):\n",
    "                    ids_to_rejoin.append(i)\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "                elif re.search(r\"\\+\", bert_tok) and re.search(r\"[0-9]+\", bert_toks[i+1]):\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "        else:\n",
    "            if re.search(r'#+\\w+', bert_tok) and not re.search(r'#+\\w+', bert_toks[i-1]) and re.search(r'#+\\w+', bert_toks[i-2]):\n",
    "                ids_to_rejoin.append([i-1, i])\n",
    "            elif re.search(r'#+\\w+', bert_tok):\n",
    "                ids_to_rejoin.append(i)\n",
    "            if re.search(r\"'\", bert_tok) and (re.search(r'what|who|why|where|how', bert_toks[i-1])):\n",
    "                if i < len(bert_toks)-1:\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "            elif re.search(r\"-|/|'\", bert_tok) and not (re.search(r'what|who|why|where|how', bert_toks[i-1])):\n",
    "                ids_to_rejoin.append(i)\n",
    "                if i < len(bert_toks)-1:\n",
    "                    ids_to_rejoin.append(i+1)\n",
    "    \n",
    "    if len(ids_to_rejoin) > 0:\n",
    "        chunk_ids = chunk_indexes(ids_to_rejoin)\n",
    "        if len(bert_embeddings) > 0:\n",
    "            new_ids, new_toks, new_embeddings, new_id = retokenizer.retokenize(bert_toks, \n",
    "                                                                               bert_ids, \n",
    "                                                                               bert_embeddings,\n",
    "                                                                               chunk_ids,\n",
    "                                                                               arbitrary_id)\n",
    "            return new_ids, new_toks, new_embeddings, new_id\n",
    "        else:\n",
    "            \n",
    "            new_ids, new_toks, new_id = retokenizer.retokenize(bert_toks, \n",
    "                                                               bert_ids, \n",
    "                                                               bert_embeddings,\n",
    "                                                               chunk_ids,\n",
    "                                                               arbitrary_id)\n",
    "            return new_ids, new_toks, new_id\n",
    "    else:\n",
    "        if len(bert_embeddings) > 0:\n",
    "            return bert_ids, bert_toks, bert_embeddings, arbitrary_id\n",
    "        else:\n",
    "            return bert_ids, bert_toks, arbitrary_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:45:39.310602Z",
     "start_time": "2019-09-04T05:45:39.293556Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: investigate how you can make this function work properly \n",
    "#FOR NOW: just use token representations of last hidden layer (implemented in cell below)\n",
    "def get_summed_embeddings(model, toks_ids, segment_ids):\n",
    "    \"\"\"\n",
    "        Input: BertModel, token id tensors, segment id tensors\n",
    "        Computation: Convert the hidden state embeddings into single token vectors\n",
    "                     Holds the list of 12 layer embeddings for each token\n",
    "                     Will have the shape: [# tokens, # layers, # features]\n",
    "        Output: Bert context embedding for each token in the question.\n",
    "                Final token embedding is the sum over the last four hidden layer representions.\n",
    "    \"\"\"\n",
    "    # encoded_layers = model(toks_ids, segment_ids)[0]\n",
    "    encoded_layers, _ = model(toks_ids)[-2:]\n",
    "    token_embeddings = np.zeros((len(toks_ids[0]), 768), dtype=float)\n",
    "    hidden_size = 12\n",
    "    batch_i = 0\n",
    "    for token_i in range(len(toks_ids[0])):\n",
    "        # 12 layers of hidden states for each token\n",
    "        hidden_layers = np.zeros((hidden_size, 768), dtype=float) \n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            hidden_layers[layer_i] = encoded_layers[layer_i][batch_i][token_i]\n",
    "        # each token's embedding is represented as a sum of the last four hidden layers\n",
    "        token_embeddings[token_i] = torch.sum(torch.stack(hidden_layers[token_i])[-4:], 0).numpy()\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:45:58.416400Z",
     "start_time": "2019-09-04T05:45:58.406372Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_token_ids(tok_questions, tok_ids, sql_data, arbitrary_id = 99999):\n",
    "    rejoined_toks = list()\n",
    "    rejoined_ids = list()\n",
    "    for i, (question, tok_q, tok_id) in enumerate(zip(sql_data, tok_questions, tok_ids)):\n",
    "        tok_id = list(tok_id[0].numpy())\n",
    "        new_ids, new_toks, new_id = merge_token_ids_embed(tok_q, tok_id, arbitrary_id, merge=None)\n",
    "        arbitrary_id = new_id  \n",
    "        rejoined_toks.append(new_toks)\n",
    "        rejoined_ids.append(new_ids)\n",
    "    return rejoined_toks, rejoined_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:46:15.374483Z",
     "start_time": "2019-09-04T05:46:15.354430Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_embeddings(tok_questions, tok_ids, segment_ids, sql_data, merge, arbitrary_id = 99999,\n",
    "                    matrix = False, tensor = False):\n",
    "    \"\"\"\n",
    "        Input: torch tensors of token ids and segment ids.\n",
    "        Computation: load pre-trained BERT model (weights),\n",
    "                     put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "                     \"torch.no_grad()\" deactivates the gradient calculations, \n",
    "                     saves memory, and speeds up computation (we don't need gradients or backprop).\n",
    "        Output: dictionary that maps token ids (keys) \n",
    "                to their corresponding BERT context embeddings (values).\n",
    "    \"\"\"\n",
    "    ## TO DO: check what's necessary to ouput all hidden states\n",
    "    # model = BertModel.from_pretrained('bert-base-uncased',\n",
    "    #                              output_hidden_states=True,\n",
    "    #                              output_attentions=True)\n",
    "    \n",
    "    model = BertModel.from_pretrained('bert-base-uncased')    \n",
    "    model.eval()\n",
    "    id2embed = dict()\n",
    "    id2tok = dict()\n",
    "    #embeddings = list()\n",
    "    rejoined_toks = list()\n",
    "    rejoined_ids = list()\n",
    "    with torch.no_grad():\n",
    "        for i, (question, tok_q, tok_id, segment_id) in enumerate(zip(sql_data, tok_questions, tok_ids, segment_ids)):\n",
    "            tok_embeddings = model(tok_id, segment_id)[0][0]\n",
    "            \n",
    "            #TODO: make \"get_summed_embeddings\" function work\n",
    "            #token_embeddings = get_summed_embeddings(model, tok_id, segment_id)\n",
    "            \n",
    "            #NOTE: only use lines below, if you'd like to create an embedding matrix (or tensor)\n",
    "            #if matrix:\n",
    "            #    if tensor:\n",
    "            #        embeddings.append(token_embeddings)\n",
    "            #    else:\n",
    "            #        for bert_embedding in token_embeddings:\n",
    "            #            embeddings.append(bert_embedding.numpy())\n",
    "            \n",
    "            tok_id = list(tok_id[0].numpy())\n",
    "            new_ids, new_toks, new_embeddings, new_id = merge_token_ids_embed(tok_q, \n",
    "                                                                              tok_id, \n",
    "                                                                              arbitrary_id,\n",
    "                                                                              merge = merge,\n",
    "                                                                              bert_embeddings = tok_embeddings)\n",
    "            arbitrary_id = new_id   \n",
    "            try:\n",
    "                assert len(new_ids) == len(new_toks) == len(question['question_tok'])\n",
    "                for tok_id, bert_tok, bert_embedding in zip(new_ids, new_toks, new_embeddings):\n",
    "                    if tok_id not in id2embed:\n",
    "                        id2embed[tok_id] = bert_embedding\n",
    "                    if tok_id not in id2tok:\n",
    "                        id2tok[tok_id] = bert_tok\n",
    "            except AssertionError:\n",
    "                pass\n",
    "            \n",
    "            rejoined_toks.append(new_toks)\n",
    "            rejoined_ids.append(new_ids)\n",
    "            \n",
    "    return rejoined_toks, rejoined_ids, id2embed, id2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:46:31.304837Z",
     "start_time": "2019-09-04T05:46:31.293807Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def drop_data(tok_questions, tok_ids, sql_data, idx_to_drop):\n",
    "    k = 0\n",
    "    n_errors = len(idx_to_drop)\n",
    "    n_questions = len(sql_data)\n",
    "    for idx in idx_to_drop:\n",
    "        tok_questions.pop(idx-k)\n",
    "        tok_ids.pop(idx-k)\n",
    "        sql_data.pop(idx-k)\n",
    "        k += 1\n",
    "    assert len(sql_data) == n_questions-n_errors, 'incorrect number of erroneous questions was dropped'\n",
    "    return tok_questions, tok_ids, sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:46:47.108436Z",
     "start_time": "2019-09-04T05:46:47.091390Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_sql_data(sql_data):\n",
    "    \"\"\"\n",
    "        Input: SQL dataset\n",
    "        Output: Updated SQL dataset with bert tokens and corresponding bert ids\n",
    "                BERT tokens were rejoined into TypeSQL's gold standard tokens and\n",
    "                hence are the same\n",
    "    \"\"\"\n",
    "    tok_questions, tok_ids, _, _ = bert_preprocessing(extract_questions(sql_data))\n",
    "    tok_questions, tok_ids = bert_token_ids(tok_questions, tok_ids, sql_data)\n",
    "    idx_to_pop = list()\n",
    "    n_original_questions = len(sql_data)\n",
    "    print(\"Number of questions before computing BERT token representations:\", n_original_questions)\n",
    "    for i, (question, tok_id, tok_question) in enumerate(zip(sql_data, tok_ids, tok_questions)):\n",
    "        try:\n",
    "            assert len(question['question_tok']) == len(tok_id)  == len(tok_question)\n",
    "        except:\n",
    "            idx_to_pop.append(i)\n",
    "        \n",
    "    tok_questions, tok_ids, sql_data = drop_data(tok_questions, tok_ids, sql_data, idx_to_pop) \n",
    "    \n",
    "    for i, (question, tok_id, tok_question) in enumerate(zip(sql_data, tok_ids, tok_questions)):\n",
    "        try:\n",
    "            assert len(sql_data[i]['question_tok']) == len(tok_id)  == len(tok_question)\n",
    "            sql_data[i]['bert_tokenized_question'] = tok_question\n",
    "            sql_data[i]['bert_token_ids'] = tok_id #list(tok_id[0].numpy())\n",
    "        except:\n",
    "            raise Exception(\"Removing incorrectly rejoined questions did not work. Check function!\")\n",
    "    \n",
    "    n_removed_questions = n_original_questions-len(sql_data)\n",
    "    \n",
    "    print(\"Number of questions in pre-processed dataset (after rejoining):\", len(sql_data))\n",
    "    print(\"Questions that could not be rejoined into TypeSQL tokens:\", n_removed_questions)\n",
    "    print(\"{}% of the original questions were removed\".format(round((n_removed_questions / n_original_questions)*100, 2)))\n",
    "    print(\"SQL data has been updated with BERT ids (tokens are the same as TypeSQL's tokens)...\")\n",
    "    return sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:47:05.692001Z",
     "start_time": "2019-09-04T05:47:05.684983Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_nonequal_questions(sql_data):\n",
    "    count = 0\n",
    "    for i, question in enumerate(sql_data):\n",
    "        try:\n",
    "            assert question['question_tok'] == question['bert_tokenized_question']\n",
    "        except AssertionError:\n",
    "            sql_data.pop(i)\n",
    "            count += 1\n",
    "    print(\"{} questions had different tokens and thus were removed from dataset\".format(count))\n",
    "    return sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T05:47:23.468263Z",
     "start_time": "2019-09-04T05:47:23.460241Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bert_pipeline(sql_data_train, sql_data_val, merge='max'):\n",
    "    sql_data = concatenate_sql_data(sql_data_train, sql_data_val)\n",
    "    tok_questions, tok_ids, segment_ids, _ = bert_preprocessing(extract_questions(sql_data))\n",
    "    _, _, id2embed, id2tok = bert_embeddings(tok_questions, tok_ids, segment_ids, sql_data, merge)\n",
    "    assert len(id2embed) == len(id2tok)\n",
    "    return id2tok, id2embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:03:32.912172Z",
     "start_time": "2019-09-05T16:03:32.897133Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def save_embeddings_as_json(id2tok, id2embed, merge, dim):\n",
    "    # np.arrays have to be converted into lists to be .json serializable\n",
    "    id2embed = {int(idx):embedding.tolist() for idx, embedding in id2embed.items()}\n",
    "    id2tok = {int(idx):tok for idx, tok in id2tok.items()}\n",
    "    if merge == 'max':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedMaxFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedMax600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedMax100.json'\n",
    "        ids = './bert/id2tokMax.json'\n",
    "    elif merge == 'avg':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedMeanFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedMean600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedMean100.json'\n",
    "        ids = './bert/id2tokMean.json'\n",
    "    elif merge == 'sum':\n",
    "        if dim == 'full':\n",
    "            embeddings = './bert/id2embedSumFull.json'\n",
    "        elif dim == 600:\n",
    "            embeddings = './bert/id2embedSum600.json'\n",
    "        elif dim == 100:\n",
    "            embeddings = './bert/id2embedSum100.json'\n",
    "        ids = './bert/id2tokSum.json' \n",
    "    else:\n",
    "        raise Exception('Embeddings have to be max-pooled, averaged or summed')\n",
    "    with open(embeddings, 'w') as json_file:\n",
    "        json.dump(id2embed, json_file)\n",
    "    with open(ids, 'w') as json_file:\n",
    "        json.dump(id2tok, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:59:44.145341Z",
     "start_time": "2019-09-05T14:59:44.112253Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_bert_dicts(file_tok, file_emb):\n",
    "    with open(file_tok) as f:\n",
    "        id2tok = json.loads(f.read())\n",
    "    with open(file_emb) as f:\n",
    "        id2embed = json.loads(f.read())\n",
    "    id2tok = {int(idx):tok for idx, tok in id2tok.items()}\n",
    "    id2embed = {int(idx):np.array(embedding) for idx, embedding in id2embed.items()}\n",
    "    assert len(id2tok) == len(id2embed)\n",
    "    return id2tok, id2embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:00:26.357848Z",
     "start_time": "2019-09-05T15:00:26.339799Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reduce_dimensionality(id2embed, dims_to_keep:int):\n",
    "    \"\"\"\n",
    "    Dimensionality reduction of BERT embeddings performed through PCA.\n",
    "        Args: id2embedding dict; number of dimensions to keep (of original 768 BERT embeddings)\n",
    "        Return: id2embdding dict with reduced dimensionality embeddings specified by dims-to-keep \n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    embeddings = np.zeros((len(id2embed), 768))\n",
    "    id2embed = dict(sorted(id2embed.items(), key=lambda kv:kv[0], reverse=False))\n",
    "    for i, (idx, embedding) in enumerate(id2embed.items()):\n",
    "        ids.append(idx)\n",
    "        embeddings[i] = embedding\n",
    "        \n",
    "    pca = PCA(n_components=dims_to_keep, svd_solver='auto', random_state=42)\n",
    "    embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    id2embed_reduced = {idx:embedding for idx, embedding in zip(ids, embeddings)}\n",
    "    return id2embed_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:29:04.219921Z",
     "start_time": "2019-09-04T05:58:37.820373Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tok, id2embed = bert_pipeline(sql_data, val_sql_data , merge='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:36:45.166209Z",
     "start_time": "2019-09-04T09:33:58.219695Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, save embeddings with full (768) dimensionality\n",
    "save_embeddings_as_json(id2tok, id2embed, merge='max', dim='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:07:09.555587Z",
     "start_time": "2019-09-04T10:06:37.440888Z"
    }
   },
   "outputs": [],
   "source": [
    "id2embed100 = reduce_dimensionality(id2embed, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:03:20.354787Z",
     "start_time": "2019-09-05T16:03:20.337742Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_embeddings_as_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0d56476aa009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# second, save embeddings with reduced (e.g., 100) dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msave_embeddings_as_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid2tok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2embed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'save_embeddings_as_json' is not defined"
     ]
    }
   ],
   "source": [
    "# second, save embeddings with reduced (e.g., 100) dimensionality\n",
    "save_embeddings_as_json(id2tok, id2embed100, merge='max', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T11:19:12.624976Z",
     "start_time": "2019-09-04T11:18:55.133942Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tokavg, id2embedavg = load_bert_dicts(\"./bert/id2tokMean.json\", \"./bert/id2embedMeanFull.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:02:05.247806Z",
     "start_time": "2019-09-05T15:00:42.777500Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tokmax, id2embedmax = load_bert_dicts(\"./bert/id2tokMax.json\", \"./bert/id2embedMaxFull.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:40:06.810608Z",
     "start_time": "2019-09-05T15:39:32.786928Z"
    }
   },
   "outputs": [],
   "source": [
    "id2embed600 = reduce_dimensionality(id2embedmax, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-05T16:03:49.912Z"
    }
   },
   "outputs": [],
   "source": [
    "# second, save embeddings with reduced (e.g., 600) dimensionality\n",
    "save_embeddings_as_json(id2tokmax, id2embed600, merge='max', dim=600)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
